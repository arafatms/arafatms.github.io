---
layout: post
title: 【内核】RCU lock
date: 2022-10-11
tags: 内核
---

# RCU锁
> RCU全程：Read-Copy-Update

RCU机制要实现的目标是，读者线程没有同步开销，或者说同步开销很小，不需要额外的锁，不需要使用原子操作指令或者内存屏障指令；而把同步的任务交给写者线程，写着线程等待所有读者线程完成后才会销毁旧数据。
RCU机制的原理可以概括为RCU记录了所有指向共享数据的指针的使用者，当需要修改共享数据时，首先先创建一个副本，在副本中修改。所有读者线程离开读者临界区后，指针指向修改后的副本，并且删除旧数据。
RCU提供的接口如下：
- rcu_read_lock/rcu_read_unlock: 组成一个rcu读者临界区;
- rcu_dereference: 用于获取被RCU保护的指针，读者线程要访问RCU保护的共享数据，需要使用该函数创建一个新指针，并且指向被RCU保护的指针；
- rcu_assign_pointer: 通常用于写者线程。在写者线程完成新数据的修改后，调用该接口可以让被RCU保护的指针指向新创建的数据，发布了更新后的数据；
- synchronize_rcu: 同步等待所有现存的读者访问完成；
- call_rcu: 注册一个回调函数，当所有现存的读访问完成后，调用这个回调函数销毁旧数据；

## 经典RCU和tree RCU

RCU中有两个很重要的概念，分别是宽限期（Grace Period，GP）和静止状态(Quiescent State, QS)。
- QS：如果一个CPU处于RCU读者临界区中，说明状态是活跃的；如果时钟滴答检测到该CPU处于用户模式或者空闲状态，又或者检测到上下文切换，就可以知道离开了读者临界区。
- GP：如果所有处于读者临界区的CPU都离开了临界区，也就是至少经历了一次QS，那么认为一个GP可以结束了；

经典RCU系统中每个CPU在GP开始时，都要设置全局位图（被一个spinlock保护）的一个位，来告诉RCU系统该CPU在临界区内。这导致大型系统中CPU数量的增大导致了spinlock的抢占很激烈。

tree RCU系统中利用CPU树状结构来构建位图，每一个节点成为rcu_node,叶子结点为CPU，比如：四个CPU的系统中，构造node1(cpu0, cpu1) + node2(cpu2, cpu3),根节点为node0. 如果每一个子节点的CPU进入临界区就会设置当前node的对应子位图，如果子位图从clear状态变成set状态就会设置父节点的位图，以此类推。